alphas <- c(0, 0.1, 0.3, 0.7, 1.0)
results <- data.frame(time = 1:n, Y = Y)
# Store forecasts
forecasts <- list()
for(a in alphas) {
level_col <- paste0("level_alpha_", a)
levels <- exp_smooth(Y, a, true_level0)
results[[level_col]] <- levels
# Generate forecasts with prediction intervals
last_level <- levels[n]
forecasts[[as.character(a)]] <- simulate_forecast(last_level, a, h, sd_error, n_sim=1000, prob=0.90)
}
# Plotting
par(mfrow = c(2, 2), mar = c(4, 4, 3, 1))
# Define colors
colors <- list("0" = "gray50", "0.1" = "blue", "0.3" = "green",
"0.7" = "orange", "1" = "red")
# Plot for each alpha
plot_alphas <- c(0, 0.1, 0.3, 1.0)
for(a in plot_alphas) {
# Extend x-axis to include forecast horizon
xlim_range <- c(1, n + h)
# Calculate y-axis limits including forecasts
forecast_data <- forecasts[[as.character(a)]]
ylim_range <- range(c(Y, results[, grep("level", names(results))],
forecast_data$lower, forecast_data$upper))
# Plot observed series
plot(1:n, Y, type = "l", pch = 20, col = "black", cex = 0.8,
xlab = "Time", ylab = "Value",
main = bquote("α = " ~ .(a) ~ " with 90% Prediction Interval"),
xlim = xlim_range,
ylim = ylim_range)
# Mark the large error
abline(v = 50, lty = 2, col = "darkred", lwd = 1.5)
abline(v = n, lty = 2, col = "gray30", lwd = 1)
text(n, ylim_range[1], "Forecast", pos = 4, col = "gray30", cex=0.8)
# Add level line
level_col <- paste0("level_alpha_", a)
lines(1:n, results[[level_col]], col = colors[[as.character(a)]], lwd = 2)
# Add forecast
forecast_times <- (n+1):(n+h)
# Add 90% prediction interval (outer, lighter shade)
polygon(c(forecast_times, rev(forecast_times)),
c(forecast_data$lower, rev(forecast_data$upper)),
col = adjustcolor(colors[[as.character(a)]], alpha.f = 0.15),
border = NA)
# Add 50% prediction interval (inner, darker shade)
polygon(c(forecast_times, rev(forecast_times)),
c(forecast_data$lower_50, rev(forecast_data$upper_50)),
col = adjustcolor(colors[[as.character(a)]], alpha.f = 0.3),
border = NA)
# Add median forecast
lines(forecast_times, forecast_data$median,
col = colors[[as.character(a)]], lwd = 2.5, lty = 1)
# Connect last level to first forecast
lines(c(n, n+1), c(results[[level_col]][n], forecast_data$median[1]),
col = colors[[as.character(a)]], lwd = 2.5, lty = 1)
# Add legend
legend("topright",
legend = c("Observed", "Fitted level", "Forecast median", "50% PI", "90% PI"),
col = c("black", colors[[as.character(a)]], colors[[as.character(a)]],
colors[[as.character(a)]], colors[[as.character(a)]]),
lwd = c(1, 2, 2.5, NA, NA),
lty = c(1, 1, 1, NA, NA),
pch = c(NA, NA, NA, 15, 15),
pt.cex = c(NA, NA, NA, 2, 2),
fill = c(NA, NA, NA,
adjustcolor(colors[[as.character(a)]], alpha.f = 0.3),
adjustcolor(colors[[as.character(a)]], alpha.f = 0.15)),
border = c(NA, NA, NA, NA, NA),
cex = 0.65,
bg = "white")
}
for(a in plot_alphas) {
# Extend x-axis to include forecast horizon
xlim_range <- c(1, n + h)
# Calculate y-axis limits including forecasts
forecast_data <- forecasts[[as.character(a)]]
ylim_range <- range(c(Y, results[, grep("level", names(results))],
forecast_data$lower, forecast_data$upper))
# Plot observed series
plot(1:n, Y, type = "l", pch = 20, col = "black", cex = 0.8,
xlab = "Time", ylab = "Value",
main = bquote("α = " ~ .(a) ~ " with 90% Prediction Interval"),
xlim = xlim_range,
ylim = ylim_range)
# Mark the large error
abline(v = 50, lty = 2, col = "darkred", lwd = 1.5)
abline(v = n, lty = 2, col = "gray30", lwd = 1)
text(n, ylim_range[1], "Forecast", pos = 4, col = "gray30", cex=0.8)
# Add level line
level_col <- paste0("level_alpha_", a)
lines(1:n, results[[level_col]], col = colors[[as.character(a)]], lwd = 2)
# Add forecast
forecast_times <- (n+1):(n+h)
# Add 90% prediction interval (outer, lighter shade)
polygon(c(forecast_times, rev(forecast_times)),
c(forecast_data$lower, rev(forecast_data$upper)),
col = adjustcolor(colors[[as.character(a)]], alpha.f = 0.15),
border = NA)
# Add 50% prediction interval (inner, darker shade)
polygon(c(forecast_times, rev(forecast_times)),
c(forecast_data$lower_50, rev(forecast_data$upper_50)),
col = adjustcolor(colors[[as.character(a)]], alpha.f = 0.3),
border = NA)
# Add median forecast
lines(forecast_times, forecast_data$median,
col = colors[[as.character(a)]], lwd = 2.5, lty = 1)
# Connect last level to first forecast
lines(c(n, n+1), c(results[[level_col]][n], forecast_data$median[1]),
col = colors[[as.character(a)]], lwd = 2.5, lty = 1)
# # Add legend
# legend("topright",
#        legend = c("Observed", "Fitted level", "Forecast median", "50% PI", "90% PI"),
#        col = c("black", colors[[as.character(a)]], colors[[as.character(a)]],
#                colors[[as.character(a)]], colors[[as.character(a)]]),
#        lwd = c(1, 2, 2.5, NA, NA),
#        lty = c(1, 1, 1, NA, NA),
#        pch = c(NA, NA, NA, 15, 15),
#        pt.cex = c(NA, NA, NA, 2, 2),
#        fill = c(NA, NA, NA,
#                 adjustcolor(colors[[as.character(a)]], alpha.f = 0.3),
#                 adjustcolor(colors[[as.character(a)]], alpha.f = 0.15)),
#        border = c(NA, NA, NA, NA, NA),
#        cex = 0.65,
#        bg = "white")
}
load("~/3-process-v2/data/Gift-v1.Rdata")
mydata
# EDA
summary(mydata)
mydata <- mydata[, list(Id, Date, Price)]
mydata[, Date:=ymd(Date)]
library(data.table)
library(CLVTools)
# EDA
summary(mydata)
mydata <- mydata[, list(Id, Date, Price)]
mydata[, Date:=ymd(Date)]
#Load necessary libraries
library(bayesplot)
library(readxl)
library(copula)
library(numDeriv)
library(stats)
library(ggplot2)
library(parallel)
library(data.table)
library(Matrix)  # Load the Matrix package for sparse matrices
library(CLVTools)
library(dplyr)
library(lubridate)
# EDA
summary(mydata)
mydata <- mydata[, list(Id, Date, Price)]
mydata[, Date:=ymd(Date)]
mydata[, Id:=as.numeric(Id)]
estimation.duration <- 104
#Create CLVdata Object
clv.data <- clvdata( mydata, date.format = "ymd", time.unit = "week",  estimation.split = estimation.duration,
name.id = "Id", name.date = "Date",name.price = "Price")
summary(clv.data)
# independent: pnbd, gam gam estimation
clv.all <- pnbd(clv.data)
cbs <- clv.all@cbs
clv.gam <- gg(clv.data)
cbs2 <- clv.gam@cbs
# Customer-By-Sufficient Statistics (CBS) matrix.
head(cbs)
head(cbs2)
# arrange everything in a data frame
data <- merge(cbs, cbs2, by = "Id") %>%
dplyr::select(-x.y) %>%  # Remove column "x.y"
rename(x = x.x, t_x = t.x, T = T.cal, m_x = Spending) %>%
mutate(Id = as.numeric(Id)) %>%
arrange(Id)  # Sort data by Id
head(data)
# Evaluation of the prediction with clvtool
dt.pred <- predict(clv.all, predict.spending = clv.gam,prediction.end=104)
df
spend_gam
# store in a dataframe
spend_gam <- data.frame( Id = data$Id, spend)
# expected spending amount
hype_gam <- opt_params
spend <- predict_spending (hype_gam, data$x , data$m_x)
# Evaluation of the prediction with clvtool
dt.pred <- predict(clv.all, predict.spending = clv.gam,prediction.end=104)
dt.pred
# select relevant data from predict
df_nb_trans <- data.frame(dt.pred[, list(Id, actual.x, CET,predicted.mean.spending,predicted.total.spending, actual.total.spending)])
# Merge clv tools prediction and our results
df <- merge( predict.nb, df_nb_trans, by = "Id", all.x = TRUE)
df_nb_trans
# Merge clv tools prediction and our results
df <- merge( predict.nb, df_nb_trans, by = "Id", all.x = TRUE)
# Merge clv tools prediction and our results
df <- df_nb_trans
head(df)
spend
df
# metrics
mae <- function(x, y){return(mean(abs(x - y)))}
rmse <- function(x, y){sqrt(mean((x - y)^2))}
# rmse from clv.tool PNBD + Gamma Gamma
mae(final_df$actual.total.spending,final_df$predicted.total.spending)
rmse(final_df$actual.total.spending,final_df$predicted.total.spending)
final_df$actual.total.spending
final_df<-df
# metrics
mae <- function(x, y){return(mean(abs(x - y)))}
rmse <- function(x, y){sqrt(mean((x - y)^2))}
# mae from code PNBD + Gamma Gamma
mae(final_df$actual.total.spending,final_df$pred.spend)
rmse(final_df$actual.total.spending,final_df$pred.spend)
final_df$actual.total.spending
final_df$predicted.total.spending
# rmse from clv.tool PNBD + Gamma Gamma
mae(final_df$actual.total.spending,final_df$predicted.total.spending)
# rmse from clv.tool PNBD + Gamma Gamma
mae(final_df$actual.total.spending,final_df$predicted.total.spending)
rmse(final_df$actual.total.spending,final_df$predicted.total.spending)
# Install and load required packages
# install.packages("dfms")
library(dfms)
# Optional: for data manipulation and visualization
# install.packages("ggplot2")
# install.packages("reshape2")
library(ggplot2)
library(reshape2)
set.seed(123)
# Parameters
n_series <- 50      # number of observed time series
n_time <- 200       # number of time periods
r <- 3              # number of common factors
p <- 2              # VAR lag order for factors
# VAR coefficient matrices
Phi1 <- matrix(c(0.5, 0.1, 0.0,
0.1, 0.4, 0.1,
0.0, 0.1, 0.3), r, r)
Phi2 <- matrix(c(0.2, 0.05, 0.0,
0.05, 0.2, 0.05,
0.0, 0.05, 0.15), r, r)
# Initialize factors
factors <- matrix(0, n_time, r)
factors[1,] <- rnorm(r)
factors[2,] <- rnorm(r)
# Generate factors with VAR(2) dynamics
for(t in 3:n_time) {
factors[t,] <- Phi1 %*% factors[t-1,] + Phi2 %*% factors[t-2,] + rnorm(r, sd = 0.5)
}
# Generate factor loadings (C matrix)
C <- matrix(runif(n_series * r, -1, 1), n_series, r)
?dfms
?DFM
# Install and load required packages
# install.packages("dfms")
library(dfms)
install.packages("dfms")
# Install and load required packages
# install.packages("dfms")
library(dfms)
?dfms
?DFM
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(CLVTools)
load("Gift-v1.Rdata")
clv.gift <- clvdata(mydata,
date.format="ymd",
time.unit = "week",
estimation.split = 104, #156,
name.id = "Id",
name.date = "Date",
name.price = "Price")
plot(clv.gift)
# Remove all other covariates
covariates.dynamic<-covariates.dynamic[,c("Id", "Cov.Date")]
# (1) Holiday dummy for December and surrounding months
# Define which months should be considered as "holiday period"
covariates.dynamic[, holiday := ifelse(month(Cov.Date) %in% c(11, 12, 1), 1, 0)]
# Alternative: only December
# dt[, holiday := ifelse(month(Cov.Date) == 12, 1, 0)]
# (2) Fourier terms for dynamic regression
# Set K (number of Fourier pairs)
K <- 3  # You can adjust this
# Create time index t (starts at 0)
covariates.dynamic[, t := 0:(.N-1), by = Id]
# Generate Fourier terms for each k from 1 to K
for(k in 1:K) {
# Sine term
covariates.dynamic[, paste0("sin_", k) := sin(2 * pi * k * t / 52)]
# Cosine term
covariates.dynamic[, paste0("cos_", k) := cos(2 * pi * k * t / 52)]
}
# Check the result
head(covariates.dynamic, 20)
nam<-colnames(covariates.dynamic)[3:ncol(covariates.dynamic)]
clv.dynamic <- SetDynamicCovariates(
clv.data = clv.gift,
data.cov.life = covariates.dynamic,
data.cov.trans = covariates.dynamic,
names.cov.life =nam,
names.cov.trans = nam,
name.id = "Id",
name.date = "Cov.Date"
)
?latentAttrition
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(CLVTools)
load("Gift-v1.Rdata")
clv.gift <- clvdata(mydata,
date.format="ymd",
time.unit = "week",
estimation.split = 104, #156,
name.id = "Id",
name.date = "Date",
name.price = "Price")
plot(clv.gift)
# Remove all other covariates
covariates.dynamic<-covariates.dynamic[,c("Id", "Cov.Date")]
# (1) Holiday dummy for December and surrounding months
# Define which months should be considered as "holiday period"
covariates.dynamic[, holiday := ifelse(month(Cov.Date) %in% c(11, 12, 1), 1, 0)]
# Alternative: only December
# dt[, holiday := ifelse(month(Cov.Date) == 12, 1, 0)]
# (2) Fourier terms for dynamic regression
# Set K (number of Fourier pairs)
K <- 3  # You can adjust this
# Create time index t (starts at 0)
covariates.dynamic[, t := 0:(.N-1), by = Id]
# Generate Fourier terms for each k from 1 to K
for(k in 1:K) {
# Sine term
covariates.dynamic[, paste0("sin_", k) := sin(2 * pi * k * t / 52)]
# Cosine term
covariates.dynamic[, paste0("cos_", k) := cos(2 * pi * k * t / 52)]
}
# Check the result
head(covariates.dynamic, 20)
nam<-colnames(covariates.dynamic)[3:ncol(covariates.dynamic)]
clv.dynamic <- SetDynamicCovariates(
clv.data = clv.gift,
data.cov.life = covariates.dynamic,
data.cov.trans = covariates.dynamic,
names.cov.life =nam,
names.cov.trans = nam,
name.id = "Id",
name.date = "Cov.Date"
)
est.pnbd.K0 <- latentAttrition(
formula =~ holiday|holiday ,
family = pnbd,
data = clv.dynamic,
optimx.args = list(method = "BFGS"))
---
title: "PNBD_covariates"
```{r load-CreateObj}
plot(clv.gift)
```
setwd("~/GitHub/Forecasting")
```{r load-CreateObj}
library(data.table)
library(CLVTools)
```
We first load the gift data set
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(CLVTools)
load("Gift-v1.Rdata")
clv.gift <- clvdata(mydata,
date.format="ymd",
time.unit = "week",
estimation.split = 156, #156,
name.id = "Id",
name.date = "Date",
name.price = "Price")
plot(clv.gift)
# Remove all other covariates
covariates.dynamic<-covariates.dynamic[,c("Id", "Cov.Date")]
# (1) Holiday dummy for December and surrounding months
# Define which months should be considered as "holiday period"
covariates.dynamic[, holiday := ifelse(month(Cov.Date) %in% c(11, 12, 1), 1, 0)]
# Alternative: only December
# dt[, holiday := ifelse(month(Cov.Date) == 12, 1, 0)]
# (2) Fourier terms for dynamic regression
# Set K (number of Fourier pairs)
K <- 7  # You can adjust this
# Create time index t (starts at 0)
covariates.dynamic[, t := 0:(.N-1), by = Id]
# Generate Fourier terms for each k from 1 to K
for(k in 5:K) {
# Sine term
covariates.dynamic[, paste0("sin_", k) := sin(2 * pi * k * t / 52)]
# Cosine term
covariates.dynamic[, paste0("cos_", k) := cos(2 * pi * k * t / 52)]
}
# Check the result
head(covariates.dynamic, 20)
nam<-colnames(covariates.dynamic)[3:ncol(covariates.dynamic)]
clv.dynamic <- SetDynamicCovariates(
clv.data = clv.gift,
data.cov.life = covariates.dynamic,
data.cov.trans = covariates.dynamic,
names.cov.life =nam,
names.cov.trans = nam,
name.id = "Id",
name.date = "Cov.Date"
)
# est.pnbd.static.full <- latentAttrition(
#   formula =~.|. ,
#   family = pnbd,
#   data = clv.dynamic,
#   optimx.args = list(method = "Nelder-Mead"))
est.pnbd.K5 <- latentAttrition(
formula =~ holiday|holiday+sin_1+cos_1 + sin_2 + cos_2 + sin_3 + cos_3 + sin_4 + cos_4 + sin_5 + cos_5 ,
family = pnbd,
data = clv.dynamic,
optimx.args = list(method = "Nelder-Mead"))
clv.dynamic
# Remove all other covariates
covariates.dynamic<-covariates.dynamic[,c("Id", "Cov.Date")]
# (1) Holiday dummy for December and surrounding months
# Define which months should be considered as "holiday period"
covariates.dynamic[, holiday := ifelse(month(Cov.Date) %in% c(11, 12, 1), 1, 0)]
# Alternative: only December
# dt[, holiday := ifelse(month(Cov.Date) == 12, 1, 0)]
# (2) Fourier terms for dynamic regression
# Set K (number of Fourier pairs)
K <- 7  # You can adjust this
# Create time index t (starts at 0)
covariates.dynamic[, t := 0:(.N-1), by = Id]
# Generate Fourier terms for each k from 1 to K
for(k in 1:K) {
# Sine term
covariates.dynamic[, paste0("sin_", k) := sin(2 * pi * k * t / 52)]
# Cosine term
covariates.dynamic[, paste0("cos_", k) := cos(2 * pi * k * t / 52)]
}
# Check the result
head(covariates.dynamic, 20)
nam<-colnames(covariates.dynamic)[3:ncol(covariates.dynamic)]
clv.dynamic <- SetDynamicCovariates(
clv.data = clv.gift,
data.cov.life = covariates.dynamic,
data.cov.trans = covariates.dynamic,
names.cov.life =nam,
names.cov.trans = nam,
name.id = "Id",
name.date = "Cov.Date"
)
# est.pnbd.static.full <- latentAttrition(
#   formula =~.|. ,
#   family = pnbd,
#   data = clv.dynamic,
#   optimx.args = list(method = "Nelder-Mead"))
est.pnbd.K5 <- latentAttrition(
formula =~ holiday|holiday+sin_1+cos_1 + sin_2 + cos_2 + sin_3 + cos_3 + sin_4 + cos_4 + sin_5 + cos_5 ,
family = pnbd,
data = clv.dynamic,
optimx.args = list(method = "Nelder-Mead"))
est.pnbd.K6 <- latentAttrition(
formula =~ holiday|holiday+sin_1+cos_1 + sin_2 + cos_2 + sin_3 + cos_3 + sin_4 + cos_4 + sin_5 + cos_5 + sin_6 + cos_6 ,
family = pnbd,
data = clv.dynamic,
optimx.args = list(method = "Nelder-Mead"))
est.pnbd.K7 <- latentAttrition(
formula =~ holiday|holiday+sin_1+cos_1 + sin_2 + cos_2 + sin_3 + cos_3 + sin_4 + cos_4 + sin_5 + cos_5 + sin_6 + cos_6 + + sin_7 + cos_7 ,
family = pnbd,
data = clv.dynamic,
optimx.args = list(method = "Nelder-Mead"))
# Create a named list to store AIC values
aic_values <- setNames(
numeric(3),
paste0("K", 5:7)
)
# Get list of model objects
models <- list(
K5 = est.pnbd.K5,
K6 = est.pnbd.K6,
K7 = est.pnbd.K7
)
# Loop through models and extract AIC
for (i in seq_along(models)) {
aic_values[i] <- summary(models[[i]])$AIC
}
# Create results data frame
aic_results <- data.frame(
Model = names(aic_values),
AIC = aic_values,
Delta_AIC = aic_values - min(aic_values),
row.names = NULL
)
# Sort by AIC
aic_results <- aic_results[order(aic_results$AIC), ]
# Display results
print(aic_results, row.names = FALSE)
# Highlight the best model
cat("\n")
cat("Best model:", aic_results$Model[1],
"with AIC =", round(aic_results$AIC[1], 2), "\n")
rmse <- function(x, y){sqrt(mean((x - y)^2))}
plot(models[aic_results$Model[1]][[1]])
##Predict the GG model separately
est.gg <- spending(family = gg, data = clv.gift)
h<-104
# select a given horizon to analyse forecasting for different segment for H = 52,104,156, 208
dt.pred.cov <- predict(models[aic_results$Model[1]][[1]], predict.spending = est.gg,prediction.end=h)
#RMSE using the best model
rmse(dt.pred.cov$actual.period.spending, dt.pred.cov$predicted.period.spending)
save.image("C:/Users/nafj/OneDrive - Université de Genève/SeasonalityExperiments.RData")
