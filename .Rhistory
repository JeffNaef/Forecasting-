library(Matrix)  # Load the Matrix package for sparse matrices
library(CLVTools)
library(dplyr)
library(lubridate)
# EDA
summary(mydata)
mydata <- mydata[, list(Id, Date, Price)]
mydata[, Date:=ymd(Date)]
mydata[, Id:=as.numeric(Id)]
estimation.duration <- 104
#Create CLVdata Object
clv.data <- clvdata( mydata, date.format = "ymd", time.unit = "week",  estimation.split = estimation.duration,
name.id = "Id", name.date = "Date",name.price = "Price")
summary(clv.data)
# independent: pnbd, gam gam estimation
clv.all <- pnbd(clv.data)
cbs <- clv.all@cbs
clv.gam <- gg(clv.data)
cbs2 <- clv.gam@cbs
# Customer-By-Sufficient Statistics (CBS) matrix.
head(cbs)
head(cbs2)
# arrange everything in a data frame
data <- merge(cbs, cbs2, by = "Id") %>%
dplyr::select(-x.y) %>%  # Remove column "x.y"
rename(x = x.x, t_x = t.x, T = T.cal, m_x = Spending) %>%
mutate(Id = as.numeric(Id)) %>%
arrange(Id)  # Sort data by Id
head(data)
# Evaluation of the prediction with clvtool
dt.pred <- predict(clv.all, predict.spending = clv.gam,prediction.end=104)
df
spend_gam
# store in a dataframe
spend_gam <- data.frame( Id = data$Id, spend)
# expected spending amount
hype_gam <- opt_params
spend <- predict_spending (hype_gam, data$x , data$m_x)
# Evaluation of the prediction with clvtool
dt.pred <- predict(clv.all, predict.spending = clv.gam,prediction.end=104)
dt.pred
# select relevant data from predict
df_nb_trans <- data.frame(dt.pred[, list(Id, actual.x, CET,predicted.mean.spending,predicted.total.spending, actual.total.spending)])
# Merge clv tools prediction and our results
df <- merge( predict.nb, df_nb_trans, by = "Id", all.x = TRUE)
df_nb_trans
# Merge clv tools prediction and our results
df <- merge( predict.nb, df_nb_trans, by = "Id", all.x = TRUE)
# Merge clv tools prediction and our results
df <- df_nb_trans
head(df)
spend
df
# metrics
mae <- function(x, y){return(mean(abs(x - y)))}
rmse <- function(x, y){sqrt(mean((x - y)^2))}
# rmse from clv.tool PNBD + Gamma Gamma
mae(final_df$actual.total.spending,final_df$predicted.total.spending)
rmse(final_df$actual.total.spending,final_df$predicted.total.spending)
final_df$actual.total.spending
final_df<-df
# metrics
mae <- function(x, y){return(mean(abs(x - y)))}
rmse <- function(x, y){sqrt(mean((x - y)^2))}
# mae from code PNBD + Gamma Gamma
mae(final_df$actual.total.spending,final_df$pred.spend)
rmse(final_df$actual.total.spending,final_df$pred.spend)
final_df$actual.total.spending
final_df$predicted.total.spending
# rmse from clv.tool PNBD + Gamma Gamma
mae(final_df$actual.total.spending,final_df$predicted.total.spending)
# rmse from clv.tool PNBD + Gamma Gamma
mae(final_df$actual.total.spending,final_df$predicted.total.spending)
rmse(final_df$actual.total.spending,final_df$predicted.total.spending)
# Install and load required packages
# install.packages("dfms")
library(dfms)
# Optional: for data manipulation and visualization
# install.packages("ggplot2")
# install.packages("reshape2")
library(ggplot2)
library(reshape2)
set.seed(123)
# Parameters
n_series <- 50      # number of observed time series
n_time <- 200       # number of time periods
r <- 3              # number of common factors
p <- 2              # VAR lag order for factors
# VAR coefficient matrices
Phi1 <- matrix(c(0.5, 0.1, 0.0,
0.1, 0.4, 0.1,
0.0, 0.1, 0.3), r, r)
Phi2 <- matrix(c(0.2, 0.05, 0.0,
0.05, 0.2, 0.05,
0.0, 0.05, 0.15), r, r)
# Initialize factors
factors <- matrix(0, n_time, r)
factors[1,] <- rnorm(r)
factors[2,] <- rnorm(r)
# Generate factors with VAR(2) dynamics
for(t in 3:n_time) {
factors[t,] <- Phi1 %*% factors[t-1,] + Phi2 %*% factors[t-2,] + rnorm(r, sd = 0.5)
}
# Generate factor loadings (C matrix)
C <- matrix(runif(n_series * r, -1, 1), n_series, r)
?dfms
?DFM
# Install and load required packages
# install.packages("dfms")
library(dfms)
install.packages("dfms")
# Install and load required packages
# install.packages("dfms")
library(dfms)
?dfms
?DFM
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(CLVTools)
load("Gift-v1.Rdata")
clv.gift <- clvdata(mydata,
date.format="ymd",
time.unit = "week",
estimation.split = 104, #156,
name.id = "Id",
name.date = "Date",
name.price = "Price")
plot(clv.gift)
# Remove all other covariates
covariates.dynamic<-covariates.dynamic[,c("Id", "Cov.Date")]
# (1) Holiday dummy for December and surrounding months
# Define which months should be considered as "holiday period"
covariates.dynamic[, holiday := ifelse(month(Cov.Date) %in% c(11, 12, 1), 1, 0)]
# Alternative: only December
# dt[, holiday := ifelse(month(Cov.Date) == 12, 1, 0)]
# (2) Fourier terms for dynamic regression
# Set K (number of Fourier pairs)
K <- 3  # You can adjust this
# Create time index t (starts at 0)
covariates.dynamic[, t := 0:(.N-1), by = Id]
# Generate Fourier terms for each k from 1 to K
for(k in 1:K) {
# Sine term
covariates.dynamic[, paste0("sin_", k) := sin(2 * pi * k * t / 52)]
# Cosine term
covariates.dynamic[, paste0("cos_", k) := cos(2 * pi * k * t / 52)]
}
# Check the result
head(covariates.dynamic, 20)
nam<-colnames(covariates.dynamic)[3:ncol(covariates.dynamic)]
clv.dynamic <- SetDynamicCovariates(
clv.data = clv.gift,
data.cov.life = covariates.dynamic,
data.cov.trans = covariates.dynamic,
names.cov.life =nam,
names.cov.trans = nam,
name.id = "Id",
name.date = "Cov.Date"
)
?latentAttrition
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(CLVTools)
load("Gift-v1.Rdata")
clv.gift <- clvdata(mydata,
date.format="ymd",
time.unit = "week",
estimation.split = 104, #156,
name.id = "Id",
name.date = "Date",
name.price = "Price")
plot(clv.gift)
# Remove all other covariates
covariates.dynamic<-covariates.dynamic[,c("Id", "Cov.Date")]
# (1) Holiday dummy for December and surrounding months
# Define which months should be considered as "holiday period"
covariates.dynamic[, holiday := ifelse(month(Cov.Date) %in% c(11, 12, 1), 1, 0)]
# Alternative: only December
# dt[, holiday := ifelse(month(Cov.Date) == 12, 1, 0)]
# (2) Fourier terms for dynamic regression
# Set K (number of Fourier pairs)
K <- 3  # You can adjust this
# Create time index t (starts at 0)
covariates.dynamic[, t := 0:(.N-1), by = Id]
# Generate Fourier terms for each k from 1 to K
for(k in 1:K) {
# Sine term
covariates.dynamic[, paste0("sin_", k) := sin(2 * pi * k * t / 52)]
# Cosine term
covariates.dynamic[, paste0("cos_", k) := cos(2 * pi * k * t / 52)]
}
# Check the result
head(covariates.dynamic, 20)
nam<-colnames(covariates.dynamic)[3:ncol(covariates.dynamic)]
clv.dynamic <- SetDynamicCovariates(
clv.data = clv.gift,
data.cov.life = covariates.dynamic,
data.cov.trans = covariates.dynamic,
names.cov.life =nam,
names.cov.trans = nam,
name.id = "Id",
name.date = "Cov.Date"
)
est.pnbd.K0 <- latentAttrition(
formula =~ holiday|holiday ,
family = pnbd,
data = clv.dynamic,
optimx.args = list(method = "BFGS"))
---
title: "PNBD_covariates"
```{r load-CreateObj}
plot(clv.gift)
```
setwd("~/GitHub/Forecasting")
```{r load-CreateObj}
library(data.table)
library(CLVTools)
```
We first load the gift data set
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(CLVTools)
load("Gift-v1.Rdata")
clv.gift <- clvdata(mydata,
date.format="ymd",
time.unit = "week",
estimation.split = 156, #156,
name.id = "Id",
name.date = "Date",
name.price = "Price")
plot(clv.gift)
# Remove all other covariates
covariates.dynamic<-covariates.dynamic[,c("Id", "Cov.Date")]
# (1) Holiday dummy for December and surrounding months
# Define which months should be considered as "holiday period"
covariates.dynamic[, holiday := ifelse(month(Cov.Date) %in% c(11, 12, 1), 1, 0)]
# Alternative: only December
# dt[, holiday := ifelse(month(Cov.Date) == 12, 1, 0)]
# (2) Fourier terms for dynamic regression
# Set K (number of Fourier pairs)
K <- 7  # You can adjust this
# Create time index t (starts at 0)
covariates.dynamic[, t := 0:(.N-1), by = Id]
# Generate Fourier terms for each k from 1 to K
for(k in 5:K) {
# Sine term
covariates.dynamic[, paste0("sin_", k) := sin(2 * pi * k * t / 52)]
# Cosine term
covariates.dynamic[, paste0("cos_", k) := cos(2 * pi * k * t / 52)]
}
# Check the result
head(covariates.dynamic, 20)
nam<-colnames(covariates.dynamic)[3:ncol(covariates.dynamic)]
clv.dynamic <- SetDynamicCovariates(
clv.data = clv.gift,
data.cov.life = covariates.dynamic,
data.cov.trans = covariates.dynamic,
names.cov.life =nam,
names.cov.trans = nam,
name.id = "Id",
name.date = "Cov.Date"
)
# est.pnbd.static.full <- latentAttrition(
#   formula =~.|. ,
#   family = pnbd,
#   data = clv.dynamic,
#   optimx.args = list(method = "Nelder-Mead"))
est.pnbd.K5 <- latentAttrition(
formula =~ holiday|holiday+sin_1+cos_1 + sin_2 + cos_2 + sin_3 + cos_3 + sin_4 + cos_4 + sin_5 + cos_5 ,
family = pnbd,
data = clv.dynamic,
optimx.args = list(method = "Nelder-Mead"))
clv.dynamic
# Remove all other covariates
covariates.dynamic<-covariates.dynamic[,c("Id", "Cov.Date")]
# (1) Holiday dummy for December and surrounding months
# Define which months should be considered as "holiday period"
covariates.dynamic[, holiday := ifelse(month(Cov.Date) %in% c(11, 12, 1), 1, 0)]
# Alternative: only December
# dt[, holiday := ifelse(month(Cov.Date) == 12, 1, 0)]
# (2) Fourier terms for dynamic regression
# Set K (number of Fourier pairs)
K <- 7  # You can adjust this
# Create time index t (starts at 0)
covariates.dynamic[, t := 0:(.N-1), by = Id]
# Generate Fourier terms for each k from 1 to K
for(k in 1:K) {
# Sine term
covariates.dynamic[, paste0("sin_", k) := sin(2 * pi * k * t / 52)]
# Cosine term
covariates.dynamic[, paste0("cos_", k) := cos(2 * pi * k * t / 52)]
}
# Check the result
head(covariates.dynamic, 20)
nam<-colnames(covariates.dynamic)[3:ncol(covariates.dynamic)]
clv.dynamic <- SetDynamicCovariates(
clv.data = clv.gift,
data.cov.life = covariates.dynamic,
data.cov.trans = covariates.dynamic,
names.cov.life =nam,
names.cov.trans = nam,
name.id = "Id",
name.date = "Cov.Date"
)
# est.pnbd.static.full <- latentAttrition(
#   formula =~.|. ,
#   family = pnbd,
#   data = clv.dynamic,
#   optimx.args = list(method = "Nelder-Mead"))
est.pnbd.K5 <- latentAttrition(
formula =~ holiday|holiday+sin_1+cos_1 + sin_2 + cos_2 + sin_3 + cos_3 + sin_4 + cos_4 + sin_5 + cos_5 ,
family = pnbd,
data = clv.dynamic,
optimx.args = list(method = "Nelder-Mead"))
est.pnbd.K6 <- latentAttrition(
formula =~ holiday|holiday+sin_1+cos_1 + sin_2 + cos_2 + sin_3 + cos_3 + sin_4 + cos_4 + sin_5 + cos_5 + sin_6 + cos_6 ,
family = pnbd,
data = clv.dynamic,
optimx.args = list(method = "Nelder-Mead"))
est.pnbd.K7 <- latentAttrition(
formula =~ holiday|holiday+sin_1+cos_1 + sin_2 + cos_2 + sin_3 + cos_3 + sin_4 + cos_4 + sin_5 + cos_5 + sin_6 + cos_6 + + sin_7 + cos_7 ,
family = pnbd,
data = clv.dynamic,
optimx.args = list(method = "Nelder-Mead"))
# Create a named list to store AIC values
aic_values <- setNames(
numeric(3),
paste0("K", 5:7)
)
# Get list of model objects
models <- list(
K5 = est.pnbd.K5,
K6 = est.pnbd.K6,
K7 = est.pnbd.K7
)
# Loop through models and extract AIC
for (i in seq_along(models)) {
aic_values[i] <- summary(models[[i]])$AIC
}
# Create results data frame
aic_results <- data.frame(
Model = names(aic_values),
AIC = aic_values,
Delta_AIC = aic_values - min(aic_values),
row.names = NULL
)
# Sort by AIC
aic_results <- aic_results[order(aic_results$AIC), ]
# Display results
print(aic_results, row.names = FALSE)
# Highlight the best model
cat("\n")
cat("Best model:", aic_results$Model[1],
"with AIC =", round(aic_results$AIC[1], 2), "\n")
rmse <- function(x, y){sqrt(mean((x - y)^2))}
plot(models[aic_results$Model[1]][[1]])
##Predict the GG model separately
est.gg <- spending(family = gg, data = clv.gift)
h<-104
# select a given horizon to analyse forecasting for different segment for H = 52,104,156, 208
dt.pred.cov <- predict(models[aic_results$Model[1]][[1]], predict.spending = est.gg,prediction.end=h)
#RMSE using the best model
rmse(dt.pred.cov$actual.period.spending, dt.pred.cov$predicted.period.spending)
save.image("C:/Users/nafj/OneDrive - Université de Genève/SeasonalityExperiments.RData")
# Pareto/NBD Model Simulation
# This code simulates customer transaction data following the Pareto/NBD model
# and formats it for use with CLVTools
library(dplyr)
library(lubridate)
# Function to simulate from Pareto/NBD model
simulate_pnbd <- function(n_customers,
beta, s,      # Gamma parameters for M0 (transaction rate)
alpha, r,     # Gamma parameters for Lambda0 (dropout rate)
gamma, q,     # Gamma parameters for N (Spending rate)
p,     # Gamma parameters for Z (Spending)
start_date = as.Date("2005-01-01"),
seasonality = F ,
observation_period = 208) {
cat("Simulating", n_customers, "customers from Pareto/NBD model...\n")
# Step 1: Draw M0 from gamma distribution Gam(beta, s)
# Using shape-rate parameterization: shape=s, rate=beta
M0 <- rgamma(n_customers, shape = s, rate = beta)
cat("Step 1: Generated dropout rates M0 from Gam(", beta, ",", s, ")\n")
# Step 2: Draw Lambda0 from gamma distribution Gam(alpha, r)
# Using shape-rate parameterization: shape=r, rate=alpha
Lambda0 <- rgamma(n_customers, shape = r, rate = alpha)
cat("Step 2: Generated transaction rates Lambda0 from Gam(", alpha, ",", r, ")\n")
# Step 3: Draw Nu from gamma distribution Gam(gamma, q)
Nu <- rgamma(n_customers, shape = q, rate = gamma)
# Step 3: For each customer, draw their unobserved lifetime Omega ~ Exp(Lambda0)
Omega <- rexp(n_customers, rate = M0)
cat("Step 3: Generated customer lifetimes Omega ~ Exp(M0)\n")
# Step 4: Generate transaction time points T_j starting from zero
cat("Step 4: Generating transaction time points T_j...\n")
all_transactions <- data.frame()
for (i in 1:n_customers) {
if (i %% 500 == 0) cat("  Processing customer", i, "of", n_customers, "\n")
customer_transactions <- data.frame()
# Customer's active period is minimum of their lifetime and observation period
active_period <- min(Omega[i], observation_period)
# Generate transaction time points T_j by cumulating inter-transaction times
# Inter-transaction times T_{j-1,j} ~ Exp(Lambda0[i])
# Transaction times T_j = sum_{l=1}^j T_{l-1,l} (starting from T_0 = 0)
transaction_times <- c()  # Will store the actual transaction times T_j
current_time <- 0  # Start from T_0 = 0
repeat {
# Generate next inter-transaction time T_{j-1,j} ~ Exp(M0[i])
inter_transaction_time <- rexp(1, rate = Lambda0[i])
# Calculate next transaction time T_j = T_{j-1} + T_{j-1,j}
next_transaction_time <- current_time + inter_transaction_time
# Stop if this transaction would occur after customer death or observation end
if (next_transaction_time >= active_period) {
break
}
# Record this transaction time
transaction_times <- c(transaction_times, next_transaction_time)
current_time <- next_transaction_time
}
# Convert transaction times to dates and create transaction records
if (length(transaction_times) > 0) {
for (j in 1:length(transaction_times)) {
transaction_date <- start_date + days( round(transaction_times[j]*7))
# Generate a realistic transaction amount (need to change this!!!)
transaction_amount <- round(rgamma(1, shape = p, rate = Nu[i]), 2)
customer_transactions <- rbind(customer_transactions,
data.frame(
Id = as.character(i),
Date = transaction_date,
Price = transaction_amount
))
}
}
# Only add customers who made at least one transaction
if (nrow(customer_transactions) > 0) {
all_transactions <- rbind(all_transactions, customer_transactions)
}
}
# Format for CLVTools (matching the apparelTrans format)
all_transactions$Id <- as.character(all_transactions$Id)
all_transactions <- all_transactions[order(all_transactions$Id, all_transactions$Date), ]
cat("Simulation complete!\n")
cat("Generated", nrow(all_transactions), "transactions for",
length(unique(all_transactions$Id)), "active customers\n")
return(all_transactions)
}
# Example usage with realistic parameters
# You can adjust these parameters based on your needs
set.seed(123)  # For reproducible results
# Simulate 600 customers (matching the example data)
simulated_data <- simulate_pnbd(
n_customers = 2000,
beta = 20,      # Rate parameter for transaction rate (higher = lower transaction rates)
s = 2,         # Shape parameter for transaction rate
alpha = 8,     # Rate parameter for dropout rate (higher = higher dropout)
r = 3,         # Shape parameter for dropout rate
q=10,
gamma=2,
p=200,
start_date = as.Date("2005-01-01"),
observation_period = 208  # One year observation period
)
# Display the results
cat("\nFirst few rows of simulated data:\n")
print(head(simulated_data, 10))
cat("\nLast few rows of simulated data:\n")
print(tail(simulated_data, 5))
cat("\nData structure:\n")
str(simulated_data)
# Summary statistics
cat("\nSummary statistics:\n")
cat("Number of unique customers:", length(unique(simulated_data$Id)), "\n")
cat("Total number of transactions:", nrow(simulated_data), "\n")
cat("Date range:", as.character(min(simulated_data$Date)), "to", as.character(max(simulated_data$Date)), "\n")
cat("Price range:", round(min(simulated_data$Price), 2), "to", round(max(simulated_data$Price), 2), "\n")
# Transactions per customer distribution
transactions_per_customer <- simulated_data %>%
group_by(Id) %>%
summarise(n_transactions = n(), .groups = 'drop')
cat("Transactions per customer summary:\n")
print(summary(transactions_per_customer$n_transactions))
library(CLVTools)
clv.sim <- clvdata(simulated_data,
date.format="ymd",
time.unit = "week",
estimation.split = 104,
name.id = "Id",
name.date = "Date",
name.price = "Price")
head(simulated_data)
clv.sim <- clvdata(simulated_data,
date.format="ymd",
time.unit = "week",
estimation.split = 104,
name.id = "Id",
name.date = "Date",
name.price = "Price")
clv.sim
plot(clv.sim)
#Estimate the PNBD model
est.pnbd <- latentAttrition(family = pnbd, data=clv.sim)
est.pnbd
summary(est.pnbd)
plot(est.pnbd)
## Finally we can predict all our customers and their expected spending in [h,T+h]:
dt.pred<-predict(est.pnbd,predict.spending = est.gg,prediction.end=52)
# Now we estimate the spending Part
est.gg <- spending(family = gg, data = clv.sim)
summary(est.gg)
plot(est.gg)
## Finally we can predict all our customers and their expected spending in [h,T+h]:
dt.pred<-predict(est.pnbd,predict.spending = est.gg,prediction.end=52)
dt.pred
