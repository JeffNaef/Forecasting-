h02 |>
model(
auto = ETS(Cost),
AAA = ETS(Cost ~ error("A") + trend("A") + season("A"))
) |>
accuracy()
?accuracy
h02 |>
model(
auto = ETS(Cost),
AAA = ETS(Cost ~ error("A") + trend("A") + season("A"))
) |>
accuracy(measures = list(
point_accuracy_measures,
distribution_accuracy_measures
))
h02 |>
model(
auto = ETS(Cost),
AAA = ETS(Cost ~ error("A") + trend("A") + season("A"))
) |>
accuracy() |>
transmute(Model = .model, MAE, RMSE, MAPE, MASE, RMSSE) |>
knitr::kable(booktabs = TRUE)
h02 |>
model(
auto = ETS(Cost),
AAA = ETS(Cost ~ error("A") + trend("A") + season("A"))
) |>
accuracy()
# If we had a test set, we could also calculate the energy distance:
#accuracy(measures = list(
#  point_accuracy_measures,
#  distribution_accuracy_measures
#))
knitr::opts_chunk$set(echo = TRUE)
library(doSNOW)
library(fpp3)
library(forecast)
library(patchwork)
library(dCovTS)
set.seed(2)
google_2018 <- gafa_stock |>
filter(Symbol == "GOOG", year(Date) == 2018)
google_2018 |>
autoplot(Close) +
labs(y = "Closing stock price ($USD)")
google_2018 |>
ACF(Close) |>
autoplot()
google_2018 |>
autoplot(difference(Close)) +
labs(y = "Change in Google closing stock price ($USD)")
google_2018 |>
ACF(difference(Close)) |>
autoplot()
google_2018 |>
mutate(diff_close = difference(Close)) |>
pull(diff_close) |>
na.omit() |>
ADCFplot()
fit_ar1 <- function(y) {
n <- length(y)
# Conditional log-likelihood function
# L(phi, sigma^2) = product of N(phi * y_{t-1}, sigma^2) for t = 2, ..., n
log_likelihood <- function(params) {
phi <- params[1]
log_sigma <- params[2]  # log(sigma) for numerical stability
sigma <- exp(log_sigma)
# Conditional mean: phi * Y_{t-1}
mu <- phi * y[1:(n-1)]
# Log-likelihood: sum of log-densities
ll <- sum(dnorm(y[2:n], mean = mu, sd = sigma, log = TRUE))
return(-ll)  # Return negative for minimization
}
# Conditional log-likelihood function
# L(phi, sigma^2) = product of N(phi * y_{t-1}, sigma^2) for t = 2, ..., n
log_likelihood <- function(params) {
phi <- params[1]
log_sigma <- params[2]  # log(sigma) for numerical stability
sigma <- exp(log_sigma)
# Conditional mean: phi * Y_{t-1}
mu <- phi * y[1:(n-1)]
# Log-likelihood: sum of log-densities
ll <- sum(dnorm(y[2:n], mean = mu, sd = sigma, log = TRUE))
return(-ll)  # Return negative for minimization
}
fit_ar1 <- function(y) {
n <- length(y)
# Optimize
result <- optim(par = c(0, 0), fn = log_likelihood, method = "BFGS")
phi_hat <- result$par[1]
sigma_hat <- exp(result$par[2])
return(list(
phi = phi_hat,
sigma = sigma_hat,
loglik = -result$value,
convergence = result$convergence
))
}
# Example Usage with Google data
# Extract the differenced close prices
google_diff <- google_2018 |>
mutate(diff_close = difference(Close)) |>
pull(diff_close) |>
na.omit()
# Fit AR(1) model
ar1_model <- fit_ar1(google_diff)
google_diff
?optim
# Conditional log-likelihood function
# L(phi, sigma^2) = product of N(phi * y_{t-1}, sigma^2) for t = 2, ..., n
log_likelihood <- function(params, y) {
phi <- params[1]
log_sigma <- params[2]  # log(sigma) for numerical stability
sigma <- exp(log_sigma)
# Conditional mean: phi * Y_{t-1}
mu <- phi * y[1:(n-1)]
# Log-likelihood: sum of log-densities
ll <- sum(dnorm(y[2:n], mean = mu, sd = sigma, log = TRUE))
return(-ll)  # Return negative for minimization
}
fit_ar1 <- function(y) {
n <- length(y)
# Optimize
result <- optim(par = c(0, 0), fn = log_likelihood, y=y, method = "BFGS")
phi_hat <- result$par[1]
sigma_hat <- exp(result$par[2])
return(list(
phi = phi_hat,
sigma = sigma_hat,
loglik = -result$value,
convergence = result$convergence
))
}
ar1_model <- fit_ar1(google_diff)
debug(fit_ar1)
ar1_model <- fit_ar1(google_diff)
y
length(y)
# Conditional log-likelihood function
# L(phi, sigma^2) = product of N(phi * y_{t-1}, sigma^2) for t = 2, ..., n
log_likelihood <- function(params, y) {
n<-length(y)
phi <- params[1]
log_sigma <- params[2]  # log(sigma) for numerical stability
sigma <- exp(log_sigma)
# Conditional mean: phi * Y_{t-1}
mu <- phi * y[1:(n-1)]
# Log-likelihood: sum of log-densities
ll <- sum(dnorm(y[2:n], mean = mu, sd = sigma, log = TRUE))
return(-ll)  # Return negative for minimization
}
fit_ar1 <- function(y) {
n <- length(y)
# Optimize
result <- optim(par = c(0, 0), fn = log_likelihood, y=y, method = "BFGS")
phi_hat <- result$par[1]
sigma_hat <- exp(result$par[2])
return(list(
phi = phi_hat,
sigma = sigma_hat,
loglik = -result$value,
convergence = result$convergence
))
}
ar1_model <- fit_ar1(google_diff)
ar1_model
# Example Usage with Google data
# Extract the differenced close prices
google_diff <- google_2018 |>
mutate(diff_close = difference(Close)) |>
pull(diff_close) |>
na.omit()
# Fit AR(1) model
ar1_model <- fit_ar1(google_diff)
# Example Usage with Google data
# Extract the differenced close prices
google_diff <- google_2018 |>
mutate(diff_close = difference(Close)) |>
pull(diff_close) |>
na.omit()
# Fit AR(1) model
ar1_model <- fit_ar1(google_diff)
ar1_model
# Example Usage with Google data
# Extract the differenced close prices
google_diff <- google_2018 |>
mutate(diff_close = difference(Close)) |>
pull(diff_close) |>
na.omit()
# Fit AR(1) model
ar1_model <- fit_ar1(google_diff)
cat("AR(1) Model Results:\n")
cat("phi =", round(ar1_model$phi, 4), "\n")
cat("sigma =", round(ar1_model$sigma, 4), "\n")
cat("Log-likelihood =", round(ar1_model$loglik, 2), "\n")
google_2018$Close
# Fit AR(1) model
ar1_model <- fit_ar1(google_2018$Close)
# Example Usage with Google data
# Fit AR(1) model
ar1_model <- fit_ar1(google_2018$Close)
cat("AR(1) Model Results:\n")
cat("phi =", round(ar1_model$phi, 4), "\n")
cat("sigma =", round(ar1_model$sigma, 4), "\n")
cat("Log-likelihood =", round(ar1_model$loglik, 2), "\n")
predict_ar1 <- function(model, last_value, h = 10, n_sim = 1000,
confidence_levels = c(80, 95)) {
sims <- simulate_ar1(model, last_value, h, n_sim)
# Calculate point forecasts (mean)
point_forecast <- colMeans(sims)
# Calculate prediction intervals for specified confidence levels
lower_matrix <- matrix(NA, nrow = h, ncol = length(confidence_levels))
upper_matrix <- matrix(NA, nrow = h, ncol = length(confidence_levels))
for(i in seq_along(confidence_levels)) {
cl <- confidence_levels[i]
alpha <- (100 - cl) / 100
lower_matrix[, i] <- apply(sims, 2, quantile, probs = alpha/2)
upper_matrix[, i] <- apply(sims, 2, quantile, probs = 1 - alpha/2)
}
return(list(
point_forecast = point_forecast,
lower = lower_matrix,
upper = upper_matrix,
levels = confidence_levels,
simulations = sims
))
}
# Generate predictions
last_obs <- tail(google_2018$Close, 1)
predictions <- predict_ar1(ar1_model, last_obs, n_ahead = 10, n_sim = 1000)
predictions <- predict_ar1(ar1_model, last_obs, h = 10, n_sim = 1000)
predict_ar1 <- function(model, last_value, h = 10, n_sim = 1000,
confidence_levels = c(80, 95)) {
sims <- simulate_ar1(model, last_value, h, n_sim)
# Calculate point forecasts (mean)
point_forecast <- colMeans(sims)
# Calculate prediction intervals for specified confidence levels
lower_matrix <- matrix(NA, nrow = h, ncol = length(confidence_levels))
upper_matrix <- matrix(NA, nrow = h, ncol = length(confidence_levels))
for(i in seq_along(confidence_levels)) {
cl <- confidence_levels[i]
alpha <- (100 - cl) / 100
lower_matrix[, i] <- apply(sims, 2, quantile, probs = alpha/2)
upper_matrix[, i] <- apply(sims, 2, quantile, probs = 1 - alpha/2)
}
return(list(
point_forecast = point_forecast,
lower = lower_matrix,
upper = upper_matrix,
levels = confidence_levels,
simulations = sims
))
}
simulate_ar1 <- function(model, last_value, n_ahead = 10, n_sim = 1000) {
phi <- model$phi
sigma <- model$sigma
# Matrix to store simulations (n_sim x n_ahead)
sims <- matrix(NA, nrow = n_sim, ncol = n_ahead)
for(i in 1:n_sim) {
y_sim <- numeric(n_ahead)
y_current <- last_value
for(t in 1:n_ahead) {
# Generate next value: Y_t = phi * Y_{t-1} + epsilon_t
epsilon <- rnorm(1, mean = 0, sd = sigma)
y_sim[t] <- phi * y_current + epsilon
y_current <- y_sim[t]
}
sims[i, ] <- y_sim
}
return(sims)
}
# Generate predictions
last_obs <- tail(google_2018$Close, 1)
predictions <- predict_ar1(ar1_model, last_obs, h = 10, n_sim = 1000)
predictions
# Generate predictions
last_obs <- tail(google_2018$Close, 1)
predictions <- predict_ar1(ar1_model, last_obs, h = 10, n_sim = 1000)
# Generate predictions
last_obs <- tail(google_2018$Close, 1)
predictions <- predict_ar1(ar1_model, last_obs, h = 10, n_sim = 1000)
predictions
# higher CI
predictions$upper
predictions$simulations
head(predictions$simulations)
# Generate predictions
last_obs <- tail(google_2018$Close, 1)
predictions <- predict_ar1(ar1_model, last_obs, h = 10, n_sim = 1000)
#point forecasts (conditional expectations)
predictions$point_forecast
# lower CI
predictions$lower
# higher CI
predictions$upper
# Simulated values:
head(predictions$simulations)
#point forecasts (conditional expectations)
paste("point forecasts:")
predictions$point_forecast
# Generate predictions
last_obs <- tail(google_2018$Close, 1)
predictions <- predict_ar1(ar1_model, last_obs, h = 10, n_sim = 1000)
#point forecasts (conditional expectations)
paste("point forecasts:")
predictions$point_forecast
# lower CI
predictions$lower
# higher CI
predictions$upper
# Simulated values:
head(predictions$simulations)
# Generate predictions
last_obs <- tail(google_2018$Close, 1)
predictions <- predict_ar1(ar1_model, last_obs, h = 10, n_sim = 1000)
#point forecasts (conditional expectations)
paste("Point Forecasts:")
predictions$point_forecast
# lower CI
paste("Lower Prediction Interval:")
predictions$lower
# higher CI
paste("Upper Prediction Interval:")
predictions$upper
# Simulated values:
paste("Simulated Future Values:")
head(predictions$simulations)
create_forecast_object <- function(y, predictions, start_time = NULL) {
n <- length(y)
n_ahead <- length(predictions$point_forecast)
# If no start_time provided, assume simple indexing
if(is.null(start_time)) {
start_forecast = n + 1
end_forecast = n + n_ahead
} else {
start_forecast = start_time
end_forecast = start_time + n_ahead - 1
}
# Create forecast object structure
forecast_obj <- structure(
list(
mean = ts(predictions$point_forecast, start = start_forecast, end = end_forecast),
lower = predictions$lower,
upper = predictions$upper,
level = predictions$levels,
x = ts(y),  # original data as time series
method = "AR(1) - Manual Implementation",
fitted = fitted(ts(y)),  # simple fitted values
residuals = residuals(ts(y))  # simple residuals
),
class = "forecast"
)
return(forecast_obj)
}
ar1_forecast <- create_forecast_object(google_diff, predictions)
ar1_forecast <- create_forecast_object(google_2018$Close, predictions)
google_2018$Close
predictions
debug(create_forecast_object)
ar1_forecast <- create_forecast_object(google_2018$Close, predictions)
# Generate predictions
last_obs <- tail(google_2018$Close, 1)
predictions <- predict_ar1(ar1_model, last_obs, h = 10, n_sim = 1000)
#point forecasts (conditional expectations)
paste("Point Forecasts:")
predictions$point_forecast
# lower CI
paste("Lower Prediction Interval:")
predictions$lower
# higher CI
paste("Upper Prediction Interval:")
predictions$upper
# Simulated values:
paste("Simulated Future Values:")
head(predictions$simulations)
ar1_forecast <- create_forecast_object(google_2018$Close, predictions)
predictions
predictions$point_forecast
ts(predictions$point_forecast, start = start_forecast, end = end_forecast)
predictions$lower
predictions$upper
predictions$levels
residuals(ts(y))
y
ts(y)
fitted(ts(y))
forecast_obj <- structure(
list(
mean = ts(predictions$point_forecast, start = start_forecast, end = end_forecast),
lower = predictions$lower,
upper = predictions$upper,
level = predictions$levels,
x = ts(y),  # original data as time series
method = "AR(1) - Manual Implementation",
),
class = "forecast"
)
forecast_obj <- structure(
list(
mean = ts(predictions$point_forecast, start = start_forecast, end = end_forecast),
lower = predictions$lower,
upper = predictions$upper,
level = predictions$levels,
x = ts(y),  # original data as time series
method = "AR(1) - Manual Implementation"
),
class = "forecast"
)
create_forecast_object <- function(y, predictions, start_time = NULL) {
n <- length(y)
n_ahead <- length(predictions$point_forecast)
# If no start_time provided, assume simple indexing
if(is.null(start_time)) {
start_forecast = n + 1
end_forecast = n + n_ahead
} else {
start_forecast = start_time
end_forecast = start_time + n_ahead - 1
}
# Create forecast object structure
forecast_obj <- structure(
list(
mean = ts(predictions$point_forecast, start = start_forecast, end = end_forecast),
lower = predictions$lower,
upper = predictions$upper,
level = predictions$levels,
x = ts(y),  # original data as time series
method = "AR(1) - Manual Implementation"
),
class = "forecast"
)
return(forecast_obj)
}
ar1_forecast <- create_forecast_object(google_2018$Close, predictions)
ar1_forecast
# Display predictions
cat("\n10-step ahead predictions:\n")
for(i in 1:10) {
cat("Step", i, ": ", round(predictions$point_forecast[i], 4),
" [", round(predictions$lower[i, 2], 4), ", ",
round(predictions$upper[i, 2], 4), "]\n")
}
# Now you can use autoplot!
library(forecast)
library(ggplot2)
# Plot using autoplot
autoplot(ar1_forecast) +
labs(title = "AR(1) Forecast - Google Stock Price Differences",
y = "Differenced Close Price",
x = "Time") +
theme_minimal()
google_2018
?ARIMA
fit <- google_2018 |>
model(ARIMA(Close, pdq(1,0,0)))
fit <- google_2018 |>
model(ARIMA(Close~0+pdq(1,0,0)))
google_2018 |>
mutate(diff_close = difference(Close)) |>
pull(diff_close) |>
na.omit() |>
ADCFplot()
fit <- google_2018 |>
model(ARIMA(pull(Close)~0+pdq(1,0,0)))
fit <- google_2018 |>
model(ARIMA(Close~0+pdq(1,0,0)))
google_simple <- google_2018 |>
mutate(time_index = row_number()) |>
as_tsibble(index = time_index, regular = TRUE)
google_simple
google_simple <- google_2018 |>
mutate(time_index = row_number()) |>
as_tsibble(index = time_index, regular = TRUE)
fit <- google_simple |>
model(ARIMA(Close~0+pdq(1,0,0)))
source("~/.active-rstudio-document", echo=TRUE)
# First make the time series regular (basically map to {1,...,T})
google_simple <- google_2018 |>
mutate(time_index = row_number()) |>
as_tsibble(index = time_index, regular = TRUE)
fit <- google_simple |>
model(ARIMA(Close~0+pdq(1,0,0)))
fit |>
forecast(h=10) |>
autoplot(Close)
fit |>
forecast(h=10)
# First make the time series regular (basically map to {1,...,T})
google_simple <- google_2018 |>
mutate(time_index = row_number()) |>
as_tsibble(index = time_index, regular = TRUE)
fit <- google_simple |>
model(ARIMA(Close~0+pdq(1,0,0)))
fit |>
forecast(h=10) |>
autoplot()
google_simple
fit
fit$`ARIMA(Close ~ 0 + pdq(1, 0, 0))`
summary(fit)
# First make the time series regular (basically map to {1,...,T})
google_simple <- google_2018 |>
mutate(time_index = row_number()) |>
as_tsibble(index = time_index, regular = TRUE)
fit <- google_simple |>
model(ARIMA(Close~0+pdq(1,0,0)))
fit |>
forecast(h=10) |>
autoplot(google_simple)
ar1_forecast <- create_forecast_object(google_2018$Close, predictions)
