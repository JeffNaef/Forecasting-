if (!require("KernSmooth")) install.packages("KernSmooth")
library(energy)
library(minerva)
library(dHSIC)
library(ppcor)
library(KernSmooth)
# Create some example data with nonlinear relationships
set.seed(123)
n <- 500
x <- runif(n, -1, 1)
# Different nonlinear relationships
y1 <- x^2 + rnorm(n, 0, 0.1)       # Quadratic
y2 <- sin(x*5) + rnorm(n, 0, 0.1)  # Sinusoidal
y3 <- exp(x) + rnorm(n, 0, 0.2)    # Exponential
y4 <- rnorm(n)                     # Independent (no relationship)
# Create a sample dataset
df <- data.frame(x = x, y1 = y1, y2 = y2, y3 = y3, y4 = y4)
# Function to calculate multiple dependence measures
calc_dependence <- function(x, y) {
results <- list()
# 1. Spearman's Rank Correlation
results$spearman <- cor(x, y, method = "spearman")
# 2. Kendall's Tau
results$kendall <- cor(x, y, method = "kendall")
# 3. Distance Correlation (dcor)
results$dcor <- dcor(x, y)
# 4. Maximal Information Coefficient (MIC)
mic_result <- mine(x, y)
results$mic <- mic_result$MIC
# 5. Mutual Information (estimated using KNN)
# Function to estimate mutual information using KNN approach
mutual_info <- function(x, y, k = 3) {
# Combine data
xy <- cbind(x, y)
n <- length(x)
# Function to calculate digamma
digamma <- function(x) {
psigamma(x, 0)
}
# Calculate nearest neighbor distances
library(FNN)
nn_xy <- get.knn(xy, k = k)
nn_x <- get.knn(matrix(x, ncol = 1), k = k)
nn_y <- get.knn(matrix(y, ncol = 1), k = k)
# Calculate mutual information
mi <- digamma(k) + digamma(n) - mean(digamma(nn_x$nn.index[, k] + 1) + digamma(nn_y$nn.index[, k] + 1))
return(mi)
}
# Try to calculate mutual information
tryCatch({
results$mutual_info <- mutual_info(x, y)
}, error = function(e) {
results$mutual_info <- NA
cat("Mutual Information calculation failed:", e$message, "\n")
})
# 6. Hoeffding's D
results$hoeffding <- hoeffd(matrix(c(x, y), ncol = 2))$D[1, 2]
# 7. dHSIC (Hilbert-Schmidt Independence Criterion)
results$dhsic <- dhsic(x, y)$dHSIC
# 8. Maximal Correlation
# A rough approximation using Alternating Conditional Expectations
ace_estimate <- function(x, y, bins = 10) {
# Discretize x and y
x_disc <- cut(x, bins, labels = FALSE)
y_disc <- cut(y, bins, labels = FALSE)
# Create contingency table
cont_table <- table(x_disc, y_disc)
# Calculate joint and marginal probabilities
p_xy <- cont_table / sum(cont_table)
p_x <- rowSums(p_xy)
p_y <- colSums(p_xy)
# Calculate maximal correlation using SVD
svd_matrix <- diag(1/sqrt(p_x)) %*% p_xy %*% diag(1/sqrt(p_y))
s <- svd(svd_matrix)
# Second largest singular value is the maximal correlation
if(length(s$d) >= 2) {
return(s$d[2])
} else {
return(0)
}
}
results$maximal_corr <- ace_estimate(x, y)
return(results)
}
# Calculate dependence measures for each relationship
results_y1 <- calc_dependence(df$x, df$y1)
results_y2 <- calc_dependence(df$x, df$y2)
# Calculate dependence measures for each relationship
results_y1 <- calc_dependence(df$x, df$y1)
library(energy)
library(minerva)
library(dHSIC)
library(ppcor)
library(KernSmooth)
# Create some example data with nonlinear relationships
set.seed(123)
n <- 500
x <- runif(n, -1, 1)
# Different nonlinear relationships
y1 <- x^2 + rnorm(n, 0, 0.1)       # Quadratic
y2 <- sin(x*5) + rnorm(n, 0, 0.1)  # Sinusoidal
y3 <- exp(x) + rnorm(n, 0, 0.2)    # Exponential
y4 <- rnorm(n)                     # Independent (no relationship)
# Create a sample dataset
df <- data.frame(x = x, y1 = y1, y2 = y2, y3 = y3, y4 = y4)
# Function to calculate multiple dependence measures
calc_dependence <- function(x, y) {
results <- list()
# 1. Spearman's Rank Correlation
results$spearman <- cor(x, y, method = "spearman")
# 2. Kendall's Tau
results$kendall <- cor(x, y, method = "kendall")
# 3. Distance Correlation (dcor)
results$dcor <- dcor(x, y)
# 4. Maximal Information Coefficient (MIC)
mic_result <- mine(x, y)
results$mic <- mic_result$MIC
# 5. Mutual Information (estimated using KNN)
# Function to estimate mutual information using KNN approach
mutual_info <- function(x, y, k = 3) {
# Combine data
xy <- cbind(x, y)
n <- length(x)
# Function to calculate digamma
digamma <- function(x) {
psigamma(x, 0)
}
# Calculate nearest neighbor distances
library(FNN)
nn_xy <- get.knn(xy, k = k)
nn_x <- get.knn(matrix(x, ncol = 1), k = k)
nn_y <- get.knn(matrix(y, ncol = 1), k = k)
# Calculate mutual information
mi <- digamma(k) + digamma(n) - mean(digamma(nn_x$nn.index[, k] + 1) + digamma(nn_y$nn.index[, k] + 1))
return(mi)
}
# Try to calculate mutual information
tryCatch({
results$mutual_info <- mutual_info(x, y)
}, error = function(e) {
results$mutual_info <- NA
cat("Mutual Information calculation failed:", e$message, "\n")
})
# 6. Hoeffding's D
results$hoeffding <- hoeffd(matrix(c(x, y), ncol = 2))$D[1, 2]
# 7. dHSIC (Hilbert-Schmidt Independence Criterion)
results$dhsic <- dhsic(x, y)$dHSIC
# 8. Maximal Correlation
# A rough approximation using Alternating Conditional Expectations
ace_estimate <- function(x, y, bins = 10) {
# Discretize x and y
x_disc <- cut(x, bins, labels = FALSE)
y_disc <- cut(y, bins, labels = FALSE)
# Create contingency table
cont_table <- table(x_disc, y_disc)
# Calculate joint and marginal probabilities
p_xy <- cont_table / sum(cont_table)
p_x <- rowSums(p_xy)
p_y <- colSums(p_xy)
# Calculate maximal correlation using SVD
svd_matrix <- diag(1/sqrt(p_x)) %*% p_xy %*% diag(1/sqrt(p_y))
s <- svd(svd_matrix)
# Second largest singular value is the maximal correlation
if(length(s$d) >= 2) {
return(s$d[2])
} else {
return(0)
}
}
results$maximal_corr <- ace_estimate(x, y)
return(results)
}
library(Hmisc)
# Calculate dependence measures for each relationship
results_y1 <- calc_dependence(df$x, df$y1)
results_y2 <- calc_dependence(df$x, df$y2)
results_y3 <- calc_dependence(df$x, df$y3)
results_y4 <- calc_dependence(df$x, df$y4)
# Create a summary data frame
summary_df <- data.frame(
Relationship = c("Quadratic", "Sinusoidal", "Exponential", "Independent"),
Spearman = c(results_y1$spearman, results_y2$spearman,
results_y3$spearman, results_y4$spearman),
Kendall = c(results_y1$kendall, results_y2$kendall,
results_y3$kendall, results_y4$kendall),
DistCorr = c(results_y1$dcor, results_y2$dcor,
results_y3$dcor, results_y4$dcor),
MIC = c(results_y1$mic, results_y2$mic,
results_y3$mic, results_y4$mic),
MutualInfo = c(results_y1$mutual_info, results_y2$mutual_info,
results_y3$mutual_info, results_y4$mutual_info),
Hoeffding = c(results_y1$hoeffding, results_y2$hoeffding,
results_y3$hoeffding, results_y4$hoeffding),
HSIC = c(results_y1$dhsic, results_y2$dhsic,
results_y3$dhsic, results_y4$dhsic),
MaxCorr = c(results_y1$maximal_corr, results_y2$maximal_corr,
results_y3$maximal_corr, results_y4$maximal_corr)
)
# Print results
print(summary_df, digits = 3)
# Print results
print(summary_df, digits = 3)
# Visualize the relationships
par(mfrow = c(2, 2))
plot(df$x, df$y1, main = "Quadratic Relationship", xlab = "X", ylab = "Y1")
plot(df$x, df$y3, main = "Exponential Relationship", xlab = "X", ylab = "Y3")
# Visualize the relationships
par(mfrow = c(2, 2))
plot(df$x, df$y1, main = "Quadratic Relationship", xlab = "X", ylab = "Y1")
plot(df$x, df$y2, main = "Sinusoidal Relationship", xlab = "X", ylab = "Y2")
plot(df$x, df$y3, main = "Exponential Relationship", xlab = "X", ylab = "Y3")
plot(df$x, df$y4, main = "No Relationship", xlab = "X", ylab = "Y4")
# Compare measure performance
# Create a heatmap of measure values
library(ggplot2)
library(reshape2)
measure_data <- summary_df[, -1]  # Remove the Relationship column
rownames(measure_data) <- summary_df$Relationship
measure_data_melted <- melt(as.matrix(measure_data))
colnames(measure_data_melted) <- c("Relationship", "Measure", "Value")
ggplot(measure_data_melted, aes(x = Measure, y = Relationship, fill = Value)) +
geom_tile() +
scale_fill_gradient2(low = "white", high = "darkblue", mid = "lightblue",
midpoint = 0.5, limit = c(0, 1)) +
theme_minimal() +
labs(title = "Comparison of Nonlinear Dependence Measures",
x = "Measure", y = "Relationship Type", fill = "Value") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
plot(df$x, df$y1, main = "Quadratic Relationship", xlab = "X", ylab = "Y1")
plot(df$x, df$y2, main = "Sinusoidal Relationship", xlab = "X", ylab = "Y2")
plot(df$x, df$y3, main = "Exponential Relationship", xlab = "X", ylab = "Y3")
plot(df$x, df$y4, main = "No Relationship", xlab = "X", ylab = "Y4")
# Calculate dependence measures for each relationship
results_y1 <- calc_dependence(df$x, df$y1)
results_y2 <- calc_dependence(df$x, df$y2)
results_y3 <- calc_dependence(df$x, df$y3)
results_y4 <- calc_dependence(df$x, df$y4)
# Create a summary data frame
summary_df <- data.frame(
Relationship = c("Quadratic", "Sinusoidal", "Exponential", "Independent"),
Spearman = c(results_y1$spearman, results_y2$spearman,
results_y3$spearman, results_y4$spearman),
Kendall = c(results_y1$kendall, results_y2$kendall,
results_y3$kendall, results_y4$kendall),
DistCorr = c(results_y1$dcor, results_y2$dcor,
results_y3$dcor, results_y4$dcor),
MIC = c(results_y1$mic, results_y2$mic,
results_y3$mic, results_y4$mic),
MutualInfo = c(results_y1$mutual_info, results_y2$mutual_info,
results_y3$mutual_info, results_y4$mutual_info),
Hoeffding = c(results_y1$hoeffding, results_y2$hoeffding,
results_y3$hoeffding, results_y4$hoeffding),
HSIC = c(results_y1$dhsic, results_y2$dhsic,
results_y3$dhsic, results_y4$dhsic),
MaxCorr = c(results_y1$maximal_corr, results_y2$maximal_corr,
results_y3$maximal_corr, results_y4$maximal_corr)
)
# Print results
print(summary_df, digits = 3)
M=matrix(c(1,3,0, 0,-2,0, 0, -3,1), byrow = T, ncol=3, nrow=3)
M
det(M)
y=1/sqrt(2)
x=1/sqrt(2)
y*(6*x+4*x^3)
y*(-6*x+4*x^3)
Sys.setenv(PATH = paste("C:/Rtools/bin", Sys.getenv("PATH"), sep=";"))
Sys.setenv(BINPREF = "C:/Rtools/mingw_$(WIN)/bin/")
install.packages("dCovTS")
# Load required libraries
library(ggplot2)
library(dCovTS)
library(forecast)
library(dplyr)
library(gridExtra)
library(seasonal)
library(astsa)
library(fpp3)
# Set seed for reproducibility
set.seed(1)
fit <- us_change |>
model(
aicc = VAR(vars(Consumption, Income)),
bic = VAR(vars(Consumption, Income), ic = "bic")
)
glance(fit)
fit |>
augment() |>
ACF(.innov) |>
autoplot()
fit |>
select(aicc) |>
forecast() |>
autoplot(us_change |> filter(year(Quarter) > 2010))
library(drf)
source("drfown.R")
source("drfown.R")
setwd("~/GitHub/Forecasting")
source("drfown.R")
n<-length(us_change$Consumption)
Yall<-as.matrix(us_change[,c("Consumption", "Income")])
dCovTS::mADCFplot(Yall)
X<-as.matrix(Yall[1:(n-1),])
Y<-as.matrix(Yall[2:n,])
fit<-drfown(X=X, Y=Y)
B<-100
p<-ncol(Yall)
Ypred<-list() ##list of length H+1, each element contains B replicate of the h prediction step
Ypred[[1]]<-matrix(Yall[n,],nrow=B, ncol=p, byrow = T) ## each element of Ypred contains
colnames(Ypred[[1]])<-colnames(Yall)
##Predict a path
for (h in 2:(H+1)){
# start from last (h-1) prediction in the same "world" b
DRFw <- predict(fit, newdata =Ypred[[h-1]] )$weights
## Draw path
sig<-abs(n^(-1/5)/drf:::medianHeuristic(Y))
Ypred[[h]]<-t(apply(DRFw,1, function(w){
mean<-Y[sample(1:nrow(Y), size=1, replace = T, w),]
return(mvrnorm(n=1, mu = mean, Sigma = sig^2 * diag(p)))
}
))
}
H<-10
fit<-drfown(X=X, Y=Y)
B<-100
p<-ncol(Yall)
Ypred<-list() ##list of length H+1, each element contains B replicate of the h prediction step
Ypred[[1]]<-matrix(Yall[n,],nrow=B, ncol=p, byrow = T) ## each element of Ypred contains
colnames(Ypred[[1]])<-colnames(Yall)
##Predict a path
for (h in 2:(H+1)){
# start from last (h-1) prediction in the same "world" b
DRFw <- predict(fit, newdata =Ypred[[h-1]] )$weights
## Draw path
sig<-abs(n^(-1/5)/drf:::medianHeuristic(Y))
Ypred[[h]]<-t(apply(DRFw,1, function(w){
mean<-Y[sample(1:nrow(Y), size=1, replace = T, w),]
return(mvrnorm(n=1, mu = mean, Sigma = sig^2 * diag(p)))
}
))
}
library(MASS)
fit<-drfown(X=X, Y=Y)
B<-100
p<-ncol(Yall)
Ypred<-list() ##list of length H+1, each element contains B replicate of the h prediction step
Ypred[[1]]<-matrix(Yall[n,],nrow=B, ncol=p, byrow = T) ## each element of Ypred contains
colnames(Ypred[[1]])<-colnames(Yall)
##Predict a path
for (h in 2:(H+1)){
# start from last (h-1) prediction in the same "world" b
DRFw <- predict(fit, newdata =Ypred[[h-1]] )$weights
## Draw path
sig<-abs(n^(-1/5)/drf:::medianHeuristic(Y))
Ypred[[h]]<-t(apply(DRFw,1, function(w){
mean<-Y[sample(1:nrow(Y), size=1, replace = T, w),]
return(mvrnorm(n=1, mu = mean, Sigma = sig^2 * diag(p)))
}
))
}
Ypred
h <- length(Ypred)  # forecast horizon
last_period <- max(us_change$Quarter)  # get last period from your data
last_period
forecast_periods <- seq(last_period, by = "quarter", length.out = h + 1)[-1]
library(fabletools)
library(dplyr)
?seq
seq(last_period, by = "quarter", length.out = h + 1)
timeDate:::seq(last_period, by = "quarter", length.out = h + 1)
timeDate::seq(last_period, by = "quarter", length.out = h + 1)
timeSequence(last_period, by = "quarter", length.out = h + 1)
timeSequence
timeDate
library(timeDate)
timeDate:::seq.timeDate(last_period, by = "quarter", length.out = h + 1)
fit |>
select(aicc) |>
forecast()
FIT
fit
# Load required libraries
library(ggplot2)
library(dCovTS)
library(forecast)
library(dplyr)
library(gridExtra)
library(seasonal)
library(astsa)
library(fpp3)
library(MASS)
# Set seed for reproducibility
set.seed(1)
fit |>
select(aicc) |>
forecast() |>
autoplot(us_change |> filter(year(Quarter) > 2010))
fit <- us_change |>
model(
aicc = VAR(vars(Consumption, Income)),
bic = VAR(vars(Consumption, Income), ic = "bic")
)
glance(fit)
fit |>
select(aicc) |>
forecast()
fit
fit |>
select(aicc) |>
forecast() |>
autoplot(us_change |> filter(year(Quarter) > 2010))
fit |>
augment() |>
ACF(.innov) |>
autoplot()
fit |>
select(aicc) |>
forecast() |>
autoplot(us_change |> filter(year(Quarter) > 2010))
# Load required libraries
library(ggplot2)
library(dCovTS)
library(forecast)
library(dplyr)
library(gridExtra)
library(seasonal)
library(astsa)
library(fpp3)
library(MASS)
# Set seed for reproducibility
set.seed(1)
fit <- us_change |>
model(
aicc = VAR(vars(Consumption, Income)),
bic = VAR(vars(Consumption, Income), ic = "bic")
)
glance(fit)
fit |>
augment() |>
ACF(.innov) |>
autoplot()
fit |>
select(aicc) |>
forecast() |>
autoplot(us_change |> filter(year(Quarter) > 2010))
fit
fit |>
select("aicc") |>
forecast() |>
autoplot(us_change |> filter(year(Quarter) > 2010))
# Load required libraries
library(ggplot2)
library(dCovTS)
library(forecast)
library(dplyr)
library(gridExtra)
library(seasonal)
library(astsa)
library(fpp3)
library(MASS)
# Set seed for reproducibility
set.seed(1)
fit <- us_change |>
model(
aicc = VAR(vars(Consumption, Income)),
bic = VAR(vars(Consumption, Income), ic = "bic")
)
glance(fit)
fit |>
augment() |>
ACF(.innov) |>
autoplot()
fit |>
select(aicc) |>
forecast() |>
autoplot(us_change |> filter(year(Quarter) > 2010))
library(drf)
source("drfown.R")
n<-length(us_change$Consumption)
Yall<-as.matrix(us_change[,c("Consumption", "Income")])
# lag 1
X<-as.matrix(Yall[1:(n-1),])
Y<-as.matrix(Yall[2:n,])
H<-10
fit<-drfown(X=X, Y=Y)
B<-100
p<-ncol(Yall)
Ypred<-list() ##list of length H+1, each element contains B replicate of the h prediction step
Ypred[[1]]<-matrix(Yall[n,],nrow=B, ncol=p, byrow = T) ## each element of Ypred contains
colnames(Ypred[[1]])<-colnames(Yall)
##Predict a path
for (h in 2:(H+1)){
# start from last (h-1) prediction in the same "world" b
DRFw <- predict(fit, newdata =Ypred[[h-1]] )$weights
## Draw path
sig<-abs(n^(-1/5)/drf:::medianHeuristic(Y))
Ypred[[h]]<-t(apply(DRFw,1, function(w){
mean<-Y[sample(1:nrow(Y), size=1, replace = T, w),]
return(mvrnorm(n=1, mu = mean, Sigma = sig^2 * diag(p)))
}
))
}
### Continue here!!!
last_period + months(3 * (1:h))
h <- length(Ypred)  # forecast horizon
last_period <- max(us_change$Quarter)  # get last period from your data
forecast_periods <- seq(last_period, by = "quarter", length.out = h + 1)[-1]
library(lubridate)
last_period + months(3 * (1:h))
last_period
us_change$Quarter
class(us_change$Quarter)
