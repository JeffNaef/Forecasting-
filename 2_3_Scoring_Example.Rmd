---
title: "Scoring Example: Google Stock Price"
output: html_document
date: "2025-07-07"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(doSNOW)
library(fpp3)
library(forecast)
library(patchwork)
library(dCovTS)
library(scoringRules)
set.seed(2)
```

We again load the google data:

```{r}
google_2018 <- gafa_stock |>
  filter(Symbol == "GOOG", year(Date) == c(2018) )
google_2018 |>
  autoplot(Close) +
  labs(y = "Closing stock price ($USD)")
```


Now as mentioned in the lecture we have some problems with the Dates because they are irregular (no trading on weekends). However it is safe to assume that time between events (i.e. observation of prices) does \textbf{not hold information} and we do not want to model this actively. As such we simply transform the Dates to $\{1,\ldots,T\}$, as discussed in the lecture (using ``mutate''):

```{r}
google_2018 <- gafa_stock |>
  filter(Symbol == "GOOG", year(Date) == 2018) |>
  mutate(Date = row_number())

google_2018 |>
  autoplot(Close) +
  labs(y = "Closing stock price ($USD)")
```

Notice how now we have numbers instead of dates.


We see a series that seems quite eradic. To understand it better, we plot the autocorrelation function (acf):

```{r}
google_2018 |>
  ACF(Close) |>
  autoplot()
```

We see that there is a clear (linear) dependence between $Y_t$ and its lags $Y_{t-h}$, even up to $h=20$. However, let's see what happens if we use first differencing:

```{r}
google_2018 |>
  autoplot(difference(Close)) +
  labs(y = "Change in Google closing stock price ($USD)")
```


## Split into train and test data

```{r}
## Split into train and test data
n <- nrow(google_2018)
H <- 10 #20  ## How many steps ahead?

# Create train and test sets as tsibbles
train_data <- google_2018 |> 
  slice(1:(n - H))

test_data <- google_2018 |> 
  slice((n - H + 1):n)

```




## Fit and Predict an AR(1) model on data

```{r}
## Fit and Predict an AR(1) model using fable
ar1_fit <- train_data |>
  model(ar1 = AR(Close ~ order(1)))

# Forecast
ar1_forecast <- ar1_fit |>
  forecast(h = H)

# Plot
ar1_forecast |>
  autoplot(train_data) +
  autolayer(test_data, Close, color = "red", linewidth = 1) +
  labs(title = "AR(1) Forecast", y = "Close Price")


```


```{r}
# Accuracy on training data
ar1_fit |> 
  accuracy()


# Accuracy on test data, including CRPS
ar1_forecast |> 
  accuracy(test_data,measures = list(CRPS = CRPS, RMSE = RMSE, MAE = MAE))

```

We now use a ``manual'' implementation to calculate the CRPS score, using the function ``crps_sample'':

```{r}
## Manual CRPS/Energy distance values using fable's generate function
## Continue here!!

## Extract and sample from forecast distributions directly
crps_values <- numeric(H)

for(i in 1:H) {
  # Get the distribution for horizon i
  forecast_dist <- ar1_forecast$Close[[i]]
  
  # Generate N=100 samples from this distribution
  samplesi <- distributional::generate(forecast_dist, times = 100)[[1]]
  
  crps_values[i] <- crps_sample(y = test_data$Close[i], 
                                dat =samplesi)
  
}
```


```{r}
# Mean CRPS across all forecast horizons
mean_crps_AR <- mean(crps_values)

ar1_forecast |> 
  accuracy(test_data,measures = list(CRPS = CRPS, RMSE = RMSE, MAE = MAE))


mean_crps_AR
```


We see that our manual implementation of the CRPS/Energy Distance is very similar to the one provided by fable!

## Fit and Predict DRF on data

```{r}
library(drf)
source("drfown.R")

traindata <- train_data$Close
testdata <- test_data$Close
ntrain <- length(traindata)

X<-as.matrix(traindata[1:(ntrain-1)])
Y<-as.matrix(traindata[2:ntrain])

fit<-drfown(X=X, Y=Y)
B<-100

Ypred<-matrix(NaN, nrow=H+1, ncol=B)
Ypred[1,]<-rep(traindata[ntrain],B)

##Predict a path
for (b in 1:B){
  
  for (h in 2:(H+1)){
    # start from last (h-1) prediction in the same "world" b 
  DRFw <- predict(fit, newdata =Ypred[h-1,b] )$weights
  ## Draw path
  sig<-abs(ntrain^(-1/5)/drf:::medianHeuristic(Y))
  mean<-Y[sample(1:nrow(Y), size=1, replace = T, DRFw[1,])]
    
  Ypred[h,b]<-rnorm(n=1, mean = mean, sd =sig )
  
  }
  
}

#Remember that the first row of Ypred is the last observed value

point_forecasts <- rowMeans(Ypred[2:(H+1),]) #pred_matrix[, "Point Forecast"]
lower_80 <- apply(Ypred[2:(H+1),], 1, quantile, probs = 0.1 )  #pred_matrix[, "Lo 80"]
upper_80 <- apply(Ypred[2:(H+1),], 1, quantile, probs = 0.9 ) #pred_matrix[, "Hi 80"]
lower_95 <- apply(Ypred[2:(H+1),], 1, quantile, probs = 0.025 ) #pred_matrix[, "Lo 95"]
upper_95 <- apply(Ypred[2:(H+1),], 1, quantile, probs = 0.975 )

# Create a manual forecast object
manual_forecast <- structure(
  list(
    mean = ts(point_forecasts, start=ntrain+1, end=ntrain+H),
    lower = cbind(lower_80, lower_95),
    upper = cbind(upper_80, upper_95),
    level = c(80, 95),
    x = ts(traindata)  # your original data
  ),
  class = "forecast"
)

# Create a time series object for the test data
test_ts <- ts(testdata, start = ntrain + 1, end = ntrain + length(testdata))

# Plot it
autoplot(manual_forecast) +
  autolayer(test_ts, color = "red", size = 1) +
  labs(title = "DRF(1) Forecast", 
       y = "Close Price", 
       x = "Time")



crps_values_DRF <- numeric(H-1)
for(i in 1:H) {
  crps_values_DRF[i] <- crps_sample(y = testdata[i], 
                               dat = Ypred[i+1,])
}


mean_crps_DRF<-mean(crps_values_DRF)

```


```{r}
paste0("AR(1) CRPS: ", round(mean_crps_AR,2))
paste0("DRF(1) CRPS: ", round(mean_crps_DRF,2) )

```





